{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"webscraper.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["u8jS41dFrDW3","t4bqQX0prDXH","9evLH41xrDXM","y3NQ3eMWrDXQ"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Kn1ZtWP4rDWp","colab_type":"text"},"cell_type":"markdown","source":["# WEB SCRAPING\n","\n","\n","---\n","\n","\n","\n","The following code performs :\n","\n","1. .Extraction of short reviews of a specified product from Amazon.\n","1. Extraction of star-rating of each review collected.\n","2. Stores the collected data in a json object.\n"]},{"metadata":{"id":"3lPShpsSrDWt","colab_type":"text"},"cell_type":"markdown","source":["**Importing the libraries**"]},{"metadata":{"id":"qyxafP1nrDWv","colab_type":"code","colab":{}},"cell_type":"code","source":["from bs4 import BeautifulSoup as bsoup\n","import ssl\n","import json\n","import requests as rq\n","import re"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u8jS41dFrDW3","colab_type":"text"},"cell_type":"markdown","source":["##Preparing the list of web-pages to be scraped"]},{"metadata":{"id":"9IBSjRN31Ue4","colab_type":"text"},"cell_type":"markdown","source":["The below code uses :\n","\n","*   The requests library to get the web-page.\n","*   BeautifulSoup pakage to extract contents.\n","*   Regular Expression to identify the right pages/links."]},{"metadata":{"id":"UFvXp4PbrDW5","colab_type":"code","colab":{},"outputId":"dc00d43a-0d44-40d2-eb93-a232c27b03cb"},"cell_type":"code","source":["#The extraction starts with the base url (The first page of customer reviews of any product)\n","# Product of choice : Vivo-V9Pro\n","base_url = 'https://www.amazon.in/Vivo-V9Pro-Black-Snapdragon-660AIE/product-reviews/B07HLNGL6R/ref=dpx_acr_txt?showViewpoints=1'\n","\n","r = rq.get(base_url) # requesting the page\n","soup = bsoup(r.text) #initialising the BeautifulSoup Object for converting pages to text \n","\n","# Using Regular Expression to identify the page navigation buttons/links\n","page_links = soup.find_all(\"a\",href=re.compile(r'.*/Vivo-V9Pro-Black-Snapdragon-660AIE/product-reviews/B07HLNGL6R.*pageNumber=\\d'))\n","\n","# Number of pages defaulted to one incase of only one page\n","try: \n","    no = re.sub('[^0-9]','',page_links[-2].get_text())  #Gets the text/label from the second last navigation button \n","    num_pages = int(no)\n","except IndexError:\n","    num_pages = 1\n","\n","# List containing the wep-page url's to be scraped\n","url_list = [\"{}&pageNumber={}\".format(base_url, str(page)) for page in range(1, num_pages + 1)]\n","\n","#print(len(url_list)) #prints the number of pages collected"],"execution_count":0,"outputs":[{"output_type":"stream","text":["50\n"],"name":"stdout"}]},{"metadata":{"id":"t4bqQX0prDXH","colab_type":"text"},"cell_type":"markdown","source":["##Ignoring certificate errors"]},{"metadata":{"id":"M9SkxinDrDXI","colab_type":"code","colab":{}},"cell_type":"code","source":["#ignoring SSL certificate errors\n","ctx = ssl.create_default_context()\n","ctx.check_hostname = False\n","ctx.verify_mode = ssl.CERT_NONE"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9evLH41xrDXM","colab_type":"text"},"cell_type":"markdown","source":["##Creating json objects"]},{"metadata":{"id":"JGXoMaRDrDXO","colab_type":"code","colab":{}},"cell_type":"code","source":["# The code uses JSON objects to store the Data extracted from the web pages.\n","\n","product_json = {}\n","product_json['short-reviews'] = []\n","product_json['short-reviews-stars'] = [] "],"execution_count":0,"outputs":[]},{"metadata":{"id":"y3NQ3eMWrDXQ","colab_type":"text"},"cell_type":"markdown","source":["##Extracting Short-reviews and Star-rating associated with each review and saving it to a json object"]},{"metadata":{"scrolled":true,"id":"TjtUIGdrrDXS","colab_type":"code","colab":{},"outputId":"aeab7e8e-4046-461a-f0bf-37e0ef963153"},"cell_type":"code","source":["i=1  #To keep track of the Page Number\n","leng = 0 # Number of reviews collected, to identify skipped pages\n","skipped = [] #list to store the skipped page numbers\n","\n","with open('Vivo_V9_reviews_.json', 'w+') as outfile:       # JSON file to save the data\n","    \n","    for url in url_list:                                   # Loop to iterate through all web-pages\n","        \n","        #print(\"\\n++++++++++++Processing page \",i,\"/\",len(url_list))   #To print the status of each page\n","        i=i+1                                                          # To track the Page number\n","        \n","        page = rq.get(url)                 #request for pages\n","        html = page.text                   #converting each page into text\n","        soup = bsoup(html, 'html.parser')  #parsing each page to identify html tags\n","        html = soup.prettify('utf-8')      #setting the encoding \n","    \n","        # block of code to extract the short-reviews of the product    \n","        \n","        for a_tags in soup.findAll('a',\n","                                   attrs={'class': 'a-size-base a-link-normal review-title a-color-base a-text-bold'}):\n","            short_review = a_tags.text.strip()\n","            product_json['short-reviews'].append(short_review)\n","        \n","        \n","        # block of code to extract the short-review's star-rating\n","        \n","        for i_tags in soup.findAll('i',\n","                                   attrs={'data-hook': 'review-star-rating'}):\n","            for spans in i_tags.findAll('span', attrs={'class': 'a-icon-alt'}):\n","                  short_review_stars = spans.text.strip()\n","                  product_json['short-reviews-stars'].append(short_review_stars)\n","                  break \n","        \n","        # Identifies the skipped page and stores the page numbers in a list\n","        if(len(product_json['short-reviews']) == leng):\n","          #print(\"\\n===============Page \",i-1,\" skipped--\")\n","          skipped.append(i-1)\n","        leng = len(product_json['short-reviews'])\n","        #print(\"\\nCollected--------Reviews/Rating = \",len(product_json['short-reviews']),\"/\",len(product_json['short-reviews-stars']))\n","    \n","    \n","    #writes the extracted reviews in to a file              \n","    json.dump(product_json, outfile, indent=4)\n","\n","print ('\\n\\n----------Extraction completed with ', len(skipped),' pages Skipped..Check json file.----------')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","++++++++++++Processing page  1 / 50\n","\n","Collected--------Reviews/Rating =  10 / 10\n","\n","++++++++++++Processing page  2 / 50\n","\n","Collected--------Reviews/Rating =  20 / 20\n","\n","++++++++++++Processing page  3 / 50\n","\n","Collected--------Reviews/Rating =  30 / 30\n","\n","++++++++++++Processing page  4 / 50\n","\n","Collected--------Reviews/Rating =  40 / 40\n","\n","++++++++++++Processing page  5 / 50\n","\n","Collected--------Reviews/Rating =  50 / 50\n","\n","++++++++++++Processing page  6 / 50\n","\n","Collected--------Reviews/Rating =  60 / 60\n","\n","++++++++++++Processing page  7 / 50\n","\n","Collected--------Reviews/Rating =  70 / 70\n","\n","++++++++++++Processing page  8 / 50\n","\n","Collected--------Reviews/Rating =  80 / 80\n","\n","++++++++++++Processing page  9 / 50\n","\n","Collected--------Reviews/Rating =  90 / 90\n","\n","++++++++++++Processing page  10 / 50\n","\n","Collected--------Reviews/Rating =  100 / 100\n","\n","++++++++++++Processing page  11 / 50\n","\n","Collected--------Reviews/Rating =  110 / 110\n","\n","++++++++++++Processing page  12 / 50\n","\n","Collected--------Reviews/Rating =  120 / 120\n","\n","++++++++++++Processing page  13 / 50\n","\n","Collected--------Reviews/Rating =  130 / 130\n","\n","++++++++++++Processing page  14 / 50\n","\n","Collected--------Reviews/Rating =  140 / 140\n","\n","++++++++++++Processing page  15 / 50\n","\n","Collected--------Reviews/Rating =  150 / 150\n","\n","++++++++++++Processing page  16 / 50\n","\n","Collected--------Reviews/Rating =  160 / 160\n","\n","++++++++++++Processing page  17 / 50\n","\n","Collected--------Reviews/Rating =  170 / 170\n","\n","++++++++++++Processing page  18 / 50\n","\n","Collected--------Reviews/Rating =  180 / 180\n","\n","++++++++++++Processing page  19 / 50\n","\n","Collected--------Reviews/Rating =  190 / 190\n","\n","++++++++++++Processing page  20 / 50\n","\n","===============Page  20  skipped--\n","\n","Collected--------Reviews/Rating =  190 / 190\n","\n","++++++++++++Processing page  21 / 50\n","\n","Collected--------Reviews/Rating =  200 / 200\n","\n","++++++++++++Processing page  22 / 50\n","\n","Collected--------Reviews/Rating =  210 / 210\n","\n","++++++++++++Processing page  23 / 50\n","\n","===============Page  23  skipped--\n","\n","Collected--------Reviews/Rating =  210 / 210\n","\n","++++++++++++Processing page  24 / 50\n","\n","Collected--------Reviews/Rating =  220 / 220\n","\n","++++++++++++Processing page  25 / 50\n","\n","Collected--------Reviews/Rating =  230 / 230\n","\n","++++++++++++Processing page  26 / 50\n","\n","===============Page  26  skipped--\n","\n","Collected--------Reviews/Rating =  230 / 230\n","\n","++++++++++++Processing page  27 / 50\n","\n","Collected--------Reviews/Rating =  240 / 240\n","\n","++++++++++++Processing page  28 / 50\n","\n","===============Page  28  skipped--\n","\n","Collected--------Reviews/Rating =  240 / 240\n","\n","++++++++++++Processing page  29 / 50\n","\n","Collected--------Reviews/Rating =  250 / 250\n","\n","++++++++++++Processing page  30 / 50\n","\n","Collected--------Reviews/Rating =  260 / 260\n","\n","++++++++++++Processing page  31 / 50\n","\n","Collected--------Reviews/Rating =  270 / 270\n","\n","++++++++++++Processing page  32 / 50\n","\n","===============Page  32  skipped--\n","\n","Collected--------Reviews/Rating =  270 / 270\n","\n","++++++++++++Processing page  33 / 50\n","\n","Collected--------Reviews/Rating =  280 / 280\n","\n","++++++++++++Processing page  34 / 50\n","\n","Collected--------Reviews/Rating =  290 / 290\n","\n","++++++++++++Processing page  35 / 50\n","\n","Collected--------Reviews/Rating =  300 / 300\n","\n","++++++++++++Processing page  36 / 50\n","\n","Collected--------Reviews/Rating =  310 / 310\n","\n","++++++++++++Processing page  37 / 50\n","\n","===============Page  37  skipped--\n","\n","Collected--------Reviews/Rating =  310 / 310\n","\n","++++++++++++Processing page  38 / 50\n","\n","Collected--------Reviews/Rating =  320 / 320\n","\n","++++++++++++Processing page  39 / 50\n","\n","Collected--------Reviews/Rating =  330 / 330\n","\n","++++++++++++Processing page  40 / 50\n","\n","Collected--------Reviews/Rating =  340 / 340\n","\n","++++++++++++Processing page  41 / 50\n","\n","Collected--------Reviews/Rating =  350 / 350\n","\n","++++++++++++Processing page  42 / 50\n","\n","Collected--------Reviews/Rating =  360 / 360\n","\n","++++++++++++Processing page  43 / 50\n","\n","===============Page  43  skipped--\n","\n","Collected--------Reviews/Rating =  360 / 360\n","\n","++++++++++++Processing page  44 / 50\n","\n","Collected--------Reviews/Rating =  370 / 370\n","\n","++++++++++++Processing page  45 / 50\n","\n","Collected--------Reviews/Rating =  380 / 380\n","\n","++++++++++++Processing page  46 / 50\n","\n","Collected--------Reviews/Rating =  390 / 390\n","\n","++++++++++++Processing page  47 / 50\n","\n","Collected--------Reviews/Rating =  400 / 400\n","\n","++++++++++++Processing page  48 / 50\n","\n","===============Page  48  skipped--\n","\n","Collected--------Reviews/Rating =  400 / 400\n","\n","++++++++++++Processing page  49 / 50\n","\n","===============Page  49  skipped--\n","\n","Collected--------Reviews/Rating =  400 / 400\n","\n","++++++++++++Processing page  50 / 50\n","\n","===============Page  50  skipped--\n","\n","Collected--------Reviews/Rating =  400 / 400\n","\n","\n","----------Extraction completed with  10  pages Skipped..Check json file.----------\n"],"name":"stdout"}]}]}